{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Beijing Air Quality Forecasting - Enhanced Bidirectional LSTM  \n",
        "*Author: Loue Sauveur Christian (Chriss)*  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully. Ready for modeling.\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# Libraries\n",
        "# -------------------------------\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# -------------------------------\n",
        "# Configuration\n",
        "# -------------------------------\n",
        "plt.style.use('default')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully. Ready for modeling.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced submission system ready\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def save_submission(predictions, experiment_name, test_index, notes=\"\"):\n",
        "    \"\"\"\n",
        "    Save model predictions to CSV in a structured format compatible with Kaggle.\n",
        "    \n",
        "    Args:\n",
        "        predictions (array-like): Model predictions.\n",
        "        experiment_name (str): Name/ID of the experiment.\n",
        "        test_index (pd.Index or pd.DatetimeIndex): Index for test data rows.\n",
        "        notes (str, optional): Any additional notes to track.\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (fixed_filename, submission DataFrame)\n",
        "    \"\"\"\n",
        "    os.makedirs('work_submissions', exist_ok=True)\n",
        "    \n",
        "    # Ensure predictions are proper format, non-negative, integer, and NaN-safe\n",
        "    predictions = np.nan_to_num(np.maximum(np.array(predictions).flatten(), 0))\n",
        "    predictions = np.round(predictions).astype(int)\n",
        "    \n",
        "    # Convert test index to datetime strings with no leading zero in hour\n",
        "    row_ids = pd.to_datetime(test_index).strftime('%Y-%m-%d %-H:%M:%S')\n",
        "    \n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': row_ids,\n",
        "        'pm2.5': predictions\n",
        "    })\n",
        "    \n",
        "    # Sort by 'row ID' for consistency\n",
        "    submission = submission.sort_values(by='row ID')\n",
        "    \n",
        "    # Fixed filename for Kaggle submission\n",
        "    fixed_filename = 'work_submissions/submission_final.csv'\n",
        "    submission.to_csv(fixed_filename, index=False)\n",
        "    \n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"Submission saved: {fixed_filename}\")\n",
        "    print(f\"Predictions - Min: {predictions.min()}, Max: {predictions.max()}, Mean: {predictions.mean():.1f}\")\n",
        "    if notes:\n",
        "        print(f\"Notes: {notes}\")\n",
        "    \n",
        "    return fixed_filename, submission\n",
        "\n",
        "print(\"✅ Enhanced submission system ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Training data shape: (30676, 12)\n",
            "Test data shape: (13148, 11)\n",
            "Train time range: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Test time range: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n",
            "Handling missing values...\n",
            "Missing values after cleaning: 0, Test: 0\n",
            "Cleaned datasets: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# Load datasets\n",
        "# -------------------------------\n",
        "print(\"Loading datasets...\")\n",
        "train = pd.read_csv('../data/train.csv')\n",
        "test = pd.read_csv('../data/test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train.shape}\")\n",
        "print(f\"Test data shape: {test.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Convert datetime and set as index\n",
        "# -------------------------------\n",
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "train.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)\n",
        "\n",
        "print(f\"Train time range: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Test time range: {test.index.min()} to {test.index.max()}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Handle missing values\n",
        "# -------------------------------\n",
        "print(\"Handling missing values...\")\n",
        "train_clean = train.fillna(method='ffill').fillna(method='bfill').interpolate()\n",
        "test_clean = test.fillna(method='ffill').fillna(method='bfill').interpolate()\n",
        "\n",
        "print(f\"Missing values after cleaning: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"Cleaned datasets: {train_clean.shape}, Test: {test_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Focused feature engineering functions are ready.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_focused_features(df, target_col='pm2.5'):\n",
        "    \"\"\"\n",
        "    Create focused features based on proven successful approach.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe with datetime index.\n",
        "        target_col (str): Target column for PM2.5 lag/rolling features.\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Enhanced dataframe with engineered features.\n",
        "    \"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "    \n",
        "    # -------------------------------\n",
        "    # Temporal features\n",
        "    # -------------------------------\n",
        "    print(\"Creating temporal features...\")\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['day_of_year'] = df_enhanced.index.dayofyear\n",
        "    \n",
        "    # Cyclical encoding\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['doy_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_year'] / 365)\n",
        "    df_enhanced['doy_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_year'] / 365)\n",
        "    \n",
        "    # -------------------------------\n",
        "    # Weather features\n",
        "    # -------------------------------\n",
        "    print(\"Creating weather interaction features...\")\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['temp_pressure'] = df_enhanced['TEMP'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_proxy'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "    \n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "    \n",
        "    # -------------------------------\n",
        "    # PM2.5 features\n",
        "    # -------------------------------\n",
        "    print(\"Creating PM2.5 lag and rolling features...\")\n",
        "    if target_col in df_enhanced.columns:\n",
        "        # Lag features\n",
        "        for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_lag_{lag}'] = df_enhanced[target_col].shift(lag)\n",
        "        # Rolling statistics\n",
        "        for window in [6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_roll_mean_{window}'] = df_enhanced[target_col].rolling(window).mean()\n",
        "            df_enhanced[f'pm2.5_roll_std_{window}'] = df_enhanced[target_col].rolling(window).std()\n",
        "        # Trend features\n",
        "        for hours in [6, 12, 24]:\n",
        "            df_enhanced[f'pm2.5_trend_{hours}h'] = df_enhanced[target_col] - df_enhanced[target_col].shift(hours)\n",
        "    \n",
        "    # -------------------------------\n",
        "    # Weather lag features\n",
        "    # -------------------------------\n",
        "    print(\"Creating weather lag features...\")\n",
        "    weather_cols = ['TEMP', 'DEWP', 'PRES', 'Iws']\n",
        "    for col in weather_cols:\n",
        "        if col in df_enhanced.columns:\n",
        "            for lag in [1, 6, 12, 24]:\n",
        "                df_enhanced[f'{col}_lag_{lag}'] = df_enhanced[col].shift(lag)\n",
        "    \n",
        "    # Drop temporary columns\n",
        "    temporal_cols = ['hour', 'day_of_week', 'month', 'day_of_year']\n",
        "    df_enhanced.drop([col for col in temporal_cols if col in df_enhanced.columns], axis=1, errors='ignore')\n",
        "    \n",
        "    print(\"Feature engineering completed.\")\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features_focused(df):\n",
        "    \"\"\"\n",
        "    Create test features without PM2.5 lag features.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Test dataframe.\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Enhanced test dataframe.\n",
        "    \"\"\"\n",
        "    df_test = create_focused_features(df)\n",
        "    df_test.drop([col for col in df_test.columns if 'pm2.5' in col], axis=1, errors='ignore', inplace=True)\n",
        "    return df_test\n",
        "\n",
        "print(\"Focused feature engineering functions are ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating focused features for train and test datasets...\n",
            "Creating temporal features...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating weather interaction features...\n",
            "Creating PM2.5 lag and rolling features...\n",
            "Creating weather lag features...\n",
            "Feature engineering completed.\n",
            "Creating temporal features...\n",
            "Creating weather interaction features...\n",
            "Creating PM2.5 lag and rolling features...\n",
            "Creating weather lag features...\n",
            "Feature engineering completed.\n",
            "Enhanced train features: 63\n",
            "Enhanced test features: 44\n",
            "Handling NaN values...\n",
            "Number of common features for modeling: 43\n",
            "Model input shapes - Train: (30676, 43), Test: (13148, 43)\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# Apply feature engineering\n",
        "# -------------------------------\n",
        "print(\"Creating focused features for train and test datasets...\")\n",
        "train_enhanced = create_focused_features(train_clean)\n",
        "test_enhanced = create_test_features_focused(test_clean)\n",
        "\n",
        "print(f\"Enhanced train features: {train_enhanced.shape[1]}\")\n",
        "print(f\"Enhanced test features: {test_enhanced.shape[1]}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Handle missing values\n",
        "# -------------------------------\n",
        "print(\"Handling NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)\n",
        "\n",
        "# -------------------------------\n",
        "# Feature alignment\n",
        "# -------------------------------\n",
        "train_feature_cols = [col for col in train_enhanced.columns if col not in ['pm2.5', 'No']]\n",
        "test_feature_cols = [col for col in test_enhanced.columns if col != 'No']\n",
        "common_features = [col for col in train_feature_cols if col in test_feature_cols]\n",
        "\n",
        "print(f\"Number of common features for modeling: {len(common_features)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Prepare model datasets\n",
        "# -------------------------------\n",
        "X_train_common = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_common = test_enhanced[common_features]\n",
        "\n",
        "print(f\"Model input shapes - Train: {X_train_common.shape}, Test: {X_test_common.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using sequence length: 48 hours\n",
            "Scaled feature shapes - Train: (30676, 43), Test: (13148, 43)\n",
            "Creating sequences for training...\n",
            "Sequence shapes - Train: (26646, 48, 43), Validation: (3982, 48, 43)\n",
            "Model setup complete and ready for training.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional,BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# -------------------------------\n",
        "# Sequence creation\n",
        "# -------------------------------\n",
        "def create_sequences(data, target, sequence_length=72):\n",
        "    \"\"\"\n",
        "    Create input sequences for LSTM models.\n",
        "    \n",
        "    Args:\n",
        "        data (np.ndarray): Feature matrix.\n",
        "        target (np.ndarray): Target values.\n",
        "        sequence_length (int): Length of each input sequence.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (X_sequences, y_values)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# -------------------------------\n",
        "# Model creation\n",
        "# -------------------------------\n",
        "def create_enhanced_bidirectional_lstm(input_shape):\n",
        "    \"\"\"\n",
        "    Build a Bidirectional LSTM model with moderate size and dropout.\n",
        "    \n",
        "    Args:\n",
        "        input_shape (tuple): Shape of input sequences.\n",
        "    \n",
        "    Returns:\n",
        "        Sequential: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(128, return_sequences=True, dropout=0.3), input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(96,activation='tanh', dropout=0.3)),\n",
        "        # BatchNormalization(),\n",
        "        Dense(64, activation='relu'),\n",
        "        # BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# Configuration\n",
        "# -------------------------------\n",
        "SEQUENCE_LENGTH = 48\n",
        "print(f\"Using sequence length: {SEQUENCE_LENGTH} hours\")\n",
        "\n",
        "# -------------------------------\n",
        "# Scaling features\n",
        "# -------------------------------\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_common)\n",
        "X_test_scaled = scaler.transform(X_test_common)\n",
        "\n",
        "print(f\"Scaled feature shapes - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Create sequences\n",
        "# -------------------------------\n",
        "print(\"Creating sequences for training...\")\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, y_train.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.87 * len(X_seq))\n",
        "X_train_seq, X_val_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\"Sequence shapes - Train: {X_train_seq.shape}, Validation: {X_val_seq.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Callbacks for training\n",
        "# -------------------------------\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),\n",
        "    ReduceLROnPlateau(factor=0.3, patience=8, min_lr=1e-4, monitor='val_loss')\n",
        "]\n",
        "print(\"Model setup complete and ready for training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Enhanced Bidirectional LSTM...\n",
            "Model parameters: 462,721\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">176,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">271,104</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m176,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │       \u001b[38;5;34m271,104\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m12,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">462,721</span> (1.77 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m462,721\u001b[0m (1.77 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">462,209</span> (1.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m462,209\u001b[0m (1.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 155ms/step - loss: 6338.3672 - mae: 53.4963 - val_loss: 4598.2725 - val_mae: 46.3127 - learning_rate: 8.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 156ms/step - loss: 4494.9253 - mae: 44.9430 - val_loss: 4459.1587 - val_mae: 44.9793 - learning_rate: 8.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 154ms/step - loss: 4124.5171 - mae: 42.9105 - val_loss: 4494.6025 - val_mae: 44.9739 - learning_rate: 8.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 155ms/step - loss: 3998.6379 - mae: 42.3676 - val_loss: 4841.6890 - val_mae: 47.2544 - learning_rate: 8.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 155ms/step - loss: 3898.8137 - mae: 41.9946 - val_loss: 4507.3882 - val_mae: 44.8468 - learning_rate: 8.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 3613.3406 - mae: 40.5856"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# Train the Enhanced Bidirectional LSTM\n",
        "# -------------------------------\n",
        "print(\"Training Enhanced Bidirectional LSTM...\")\n",
        "\n",
        "model = create_enhanced_bidirectional_lstm(X_train_seq.shape[1:])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0008, beta_1=0.8, beta_2=0.999),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(f\"Model parameters: {model.count_params():,}\")\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    epochs=40,\n",
        "    batch_size=70,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate model performance\n",
        "# -------------------------------\n",
        "val_pred = model.predict(X_val_seq, verbose=0)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val_seq, val_pred))\n",
        "\n",
        "print(\"\\nTraining completed.\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Expected public score (approx.): {val_rmse * 54:.0f}\")\n",
        "\n",
        "# Aggressive target analysis\n",
        "target_2000_rmse = 2000 / 54  # Required validation RMSE for public score of 2000\n",
        "current_expected = val_rmse * 54\n",
        "\n",
        "print(\"\\nAggressive target analysis:\")\n",
        "print(f\"Target public score: 2000\")\n",
        "print(f\"Required validation RMSE: {target_2000_rmse:.1f}\")\n",
        "print(f\"Current validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Current expected public score: {current_expected:.0f}\")\n",
        "\n",
        "# Performance assessment\n",
        "if current_expected < 2000:\n",
        "    print(\"Aggressive target achieved: Expected < 2000\")\n",
        "elif current_expected < 3000:\n",
        "    print(\"Great progress toward 2000 target\")\n",
        "elif current_expected < 3500:\n",
        "    print(\"Good improvement, more work needed for 2000\")\n",
        "else:\n",
        "    print(\"Significant improvements required to reach 2000 target\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# Generate test sequences\n",
        "# -------------------------------\n",
        "def create_test_sequences(X_test_scaled, X_train_scaled, sequence_length):\n",
        "    \"\"\"\n",
        "    Create sequences for test data based on sliding window approach.\n",
        "    \n",
        "    Args:\n",
        "        X_test_scaled (np.ndarray): Scaled test features.\n",
        "        X_train_scaled (np.ndarray): Scaled train features (for early sequences).\n",
        "        sequence_length (int): Length of input sequences.\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Test sequences ready for prediction.\n",
        "    \"\"\"\n",
        "    test_sequences = []\n",
        "    \n",
        "    for i in range(len(X_test_scaled)):\n",
        "        if i < sequence_length:\n",
        "            needed_from_train = sequence_length - (i + 1)\n",
        "            if needed_from_train > 0:\n",
        "                sequence = np.vstack([X_train_scaled[-needed_from_train:], X_test_scaled[:i+1]])\n",
        "            else:\n",
        "                sequence = X_test_scaled[:sequence_length]\n",
        "        else:\n",
        "            sequence = X_test_scaled[i-sequence_length+1:i+1]\n",
        "        test_sequences.append(sequence)\n",
        "    \n",
        "    return np.array(test_sequences)\n",
        "\n",
        "print(f\"Creating test sequences with length {SEQUENCE_LENGTH}...\")\n",
        "X_test_seq = create_test_sequences(X_test_scaled, X_train_scaled, SEQUENCE_LENGTH)\n",
        "print(f\"Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Generate predictions\n",
        "# -------------------------------\n",
        "print(\"Generating test predictions...\")\n",
        "test_predictions = model.predict(X_test_seq, verbose=1)\n",
        "test_predictions = np.maximum(test_predictions.flatten(), 0)  # Ensure non-negative\n",
        "\n",
        "print(f\"Predictions summary - Min: {test_predictions.min():.1f}, Max: {test_predictions.max():.1f}, Mean: {test_predictions.mean():.1f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Save submission\n",
        "# -------------------------------\n",
        "expected_score = int(val_rmse * 54)\n",
        "experiment_name = f\"improved_deep_lstm_rmse_{val_rmse:.0f}\"\n",
        "notes = f\"Improved Deep LSTM: Val RMSE {val_rmse:.2f}, Expected {expected_score}, {len(common_features)} features\"\n",
        "\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index,\n",
        "    notes\n",
        ")\n",
        "\n",
        "print(\"\\nSubmission complete.\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Expected public score: ~{expected_score}\")\n",
        "print(f\"Submission saved: {filename}\")\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Confirm submission\n",
        "if os.path.exists(filename):\n",
        "    print(f\"Confirmed: {filename} ready for submission\")\n",
        "    print(f\"File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"Warning: {filename} not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# Training Visualization - Loue Sauveur Christian\n",
        "# -------------------------------\n",
        "print(\"📊 Training Summary - Loue Sauveur Christian\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# 1. Loss curves\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.title('Enhanced BiLSTM Loss Curves (Loue Sauveur Christian)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 2. Actual vs Predicted scatter\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(y_val_seq[:500], val_pred.flatten()[:500], alpha=0.6, c='purple')\n",
        "plt.plot([y_val_seq.min(), y_val_seq.max()], [y_val_seq.min(), y_val_seq.max()], 'r--')\n",
        "plt.xlabel('Actual PM2.5')\n",
        "plt.ylabel('Predicted PM2.5')\n",
        "plt.title('Actual vs Predicted (Top 500) - Loue Sauveur Christian')\n",
        "plt.grid(True)\n",
        "\n",
        "# 3. Histogram of prediction errors\n",
        "errors = y_val_seq.flatten() - val_pred.flatten()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(errors, bins=40, color='green', alpha=0.7)\n",
        "plt.axvline(errors.mean(), color='red', linestyle='--', label=f\"Mean Error: {errors.mean():.2f}\")\n",
        "plt.title('Prediction Error Distribution - Loue Sauveur Christian')\n",
        "plt.xlabel('Error (Actual - Predicted)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 4. Actual vs Predicted trends over time\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(y_val_seq[:200], label='Actual', color='black')\n",
        "plt.plot(val_pred.flatten()[:200], label='Predicted', color='cyan')\n",
        "plt.title('Trend Comparison (First 200 Samples) - Loue Sauveur Christian')\n",
        "plt.xlabel('Time Index')\n",
        "plt.ylabel('PM2.5')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# Summary of experiment\n",
        "# -------------------------------\n",
        "print(\"\\n📌 Experiment Summary - Aggressive Target\")\n",
        "print(f\"Model: Enhanced Bidirectional LSTM (256→128→64)\")\n",
        "print(f\"Number of features: {len(common_features)}\")\n",
        "print(f\"Sequence length: {SEQUENCE_LENGTH} hours\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Expected public score: ~{val_rmse * 54:.0f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Progress Analysis\n",
        "# -------------------------------\n",
        "previous_best = 3877.96\n",
        "target_aggressive = 2000\n",
        "current_expected = val_rmse * 54\n",
        "improvement = previous_best - current_expected\n",
        "target_gap = current_expected - target_aggressive\n",
        "\n",
        "print(\"\\n🚀 Progress toward 2000 target:\")\n",
        "print(f\"Previous best: {previous_best:.0f}\")\n",
        "print(f\"Aggressive target: {target_aggressive}\")\n",
        "print(f\"Current expected: {current_expected:.0f}\")\n",
        "print(f\"Improvement from previous: {improvement:.0f} RMSE\")\n",
        "print(f\"Gap to 2000 target: {target_gap:.0f} RMSE\")\n",
        "\n",
        "# Status assessment\n",
        "if current_expected < 2000:\n",
        "    status = \"Aggressive target achieved\"\n",
        "elif current_expected < 3000:\n",
        "    status = \"Major progress toward target\"\n",
        "elif current_expected < 3500:\n",
        "    status = \"Good progress, needs further optimization\"\n",
        "else:\n",
        "    status = \"Significant improvements required\"\n",
        "\n",
        "print(f\"Status: {status}\")\n",
        "print(\"\\n✅ Enhanced Bidirectional LSTM experiment complete - Loue Sauveur Christian.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
